\section{Introducción Teórica}

Las funciones que usaremos para aproximar la inversa de la raíz son las siguientes:

\begin{displaymath}
    f(x) = x^2 - \alpha
\end{displaymath}

\begin{displaymath}
    e(x) = \frac{1}{x^2} - \alpha
\end{displaymath}

A partir de este momento nos enfocaremos en encontrar los ceros de estas dos funciones usando los métodos de Newton y de la Secante:

\begin{displaymath}
    x_{n + 1} = x_n - \frac{h(x_n)}{h'(x_n)}
\end{displaymath}

\begin{displaymath}
    x_{n + 1} = x_{n - 1} - h(x_{n - 1})\frac{x_{n - 1} - x_{n - 2}}{h(x_{n - 1}) - h(x_{n - 2})}
\end{displaymath}

Donde $\displaystyle h(x) = f(x)$ o $\displaystyle h(x) = e(x)$

En este caso para calcular la inversa de la raíz cuadrada
($\displaystyle\frac{1}{\sqrt{\alpha}}$) buscaremos los ceros de dos funciones.

La resolución de estas 2 funciones es equivalente a resolver el problema de la
inversa de la raíz. A partir de que el problema se plantea de esa manera es que
podemos hacer uso de métodos para encontrar ceros de funciones.\\

Las funciones en cuestión son


Cada una de estas funciones provee formas de encontrar la raiz de un numero.
Veamos el caso de $f(x)$.

Los métodos elegidos para encontrar las raices de estas funciones son el método
de Newton y el método de la Secante.

\subsubsection{Análisis de las funciones}

Notemos que $\alpha$ siempre tiene que ser positivo, sino no se le puede calcular la raiz cuadrada a un numero negativo. Mas aún $\alpha > 0$ ya que sino estaríamos diviendo por cero.

\subsubsection{f(x)}

Primero demostrar que cuando $f(x) == 0 -> x == \sqrt{\alpha}$
entonces una vez encontradas las raices podemos hacer $1/x$ para encontrar
$\displaystyle\frac{1}{\sqrt{\alpha}}$.

En el caso de $\displaystyle e(x) == 0 -> x == \frac{1}/{\sqrt{\alpha}}$ por lo que no tenemos que hacer
ninguna otra cuenta.\\

Analicemos graficamente las funciones:\\

% intertart grafico lindo de f(x)

$f(x)$ es una parábola. Al ser $\alpha > 0$ podemos ver que *siempre* tiene dos
raíces. Más aún $f(x)$ es simétrica por lo cual podemos encontrar cualquiera de
las dos raíces y con esta cambiarle el signo y obtener la otra. De esta forma
no nos preocuparemos por obtener la raíz positiva ya que nos es indistinto que
raíz conseguimos con los métodos.\\

% insertar grafico lindo de e(x)

$e(x)$ con $\alpha > 0$ también tiene siempre dos raíces por lo que al igual que con
$f(x)$ nos es indistinto cuál de las dos obtenemos. En este caso es importante
notar que en el 0 hay una asíntota de las ordenadas.\\
 
Los dos métodos que elegimos trabajan con la tangente de las funciones en un
punto o con una aproximación de esta. Veamos la concavidad de las funciones.

% a partir de aca es todo chamuyo... ver que dejar y que sacar, porque en
% realida se puede hacer buen analisis analitico pero no se como.. buscar en
% google o wikipedia quizas


% graficos lindos de f''(x) y e''(x).

veamos que $f''(x) == 2$. esta funcion es convexa. al ser constante y por la
forma que tienen las derivadas asumimos que siempre va a converger.

veamos $e''(x)$ se nos va al sorete en el 0. por algo que no se, que nos tenemos
que sacar de la galera esta no siempre va a converger!!! pero no sabemos bien
por que


Cómo se puede ver los valores que hacen cero a esas funciones son de la forma
$x = \sqrt{\alpha}$ en el primer caso y $x = \frac{1}{\sqrt{\alpha}}$ en el
segundo.

\subsubsection{Métodos}
Podemos ver que se puede utilizar tanto Newton como Secante porque las
funciones cumplen lo que pida cada una.

Podemos utilizar la secante porque .. y también Newton porque pide todo esto y
que exista la derivada primera de la funcion, las cuales tenemos para $e$ y para
$f$.

\subsubsection{Hipótesis}
Creemos que Newton va a funcionar mucho mejor que Secante porque su órden de
convergencia teórico es mejor.

Por otro lado no estamos seguros de cuál de las dos funciones va a ser mejor
porque no podemos interpretar funciones y como afectan las tangentes a los
métodos.

% no se si esto viene aca o mas abajo, quizas mas abajo o arriba, pero si nos
% interesa ver los dos casos, el caso con bajo error y el caso rapido.
Por último queremos hacer un análisis para un cálculo aproximado pero MUY
RAPIDO al estilo quake. Para este puede ser que las conclusiones sean
diferentes a casos donde puede haber mas iteraciones.  por que? porque para un
caso podemos empezar mucho ma cerca de la raiz y hacer pocas iteraciones y en
el otro caso podemos empezar muy lejos pero con algunas iteraciones mas ya
estar mucho mas cerca. pero para ej 3 iteraciones es mejor el metodo y funcion
\"lento\".

\subsection{Prueba de la convergencia de Newton}
Vamos a demostrar inductivamente la convergencia del método de Newton usando la función $\displaystyle f(x) = x^2 - \alpha$\\

Lo que queremos demostrar es que si $x_0 > \sqrt{\alpha}$ entonces $x_{n + 1} < x_n$ para todo $n > 0$ donde $\displaystyle x_{n + 1} = \frac{1}{2}(x_n + \frac{\alpha}{x_n})$.\\

{\large \bf Inducción en k}\\
Queremos probar que $x_{k + 1} < x_k$.\\

{\bf Caso base: $P(0)$}\\
Sea $k = 1$, queremos ver que $x_1 < x_0$. Supongamos que no vale. Entonces $x_1 \ge x_0$. Por lo tanto:

\begin{displaymath}
    \frac{1}{2}(x_0 + \frac{\alpha}{x_0}) \ge x_0 \implies x_0 + \frac{\alpha}{x_0} \ge 2x_0 \implies \frac{\alpha}{x_0} \ge x_0 \implies \alpha \ge x_0^2 \implies x_0 \le \sqrt{\alpha}
\end{displaymath}

Pero esto es absurdo pues por hipótesis sabíamos que $x_0 > \sqrt{\alpha}$. Por lo tanto $x_1 < x_0$ $\square$\\

Veamos también que $x_1 > \sqrt{\alpha}$\\

Supongamos que no vale. Entonces $x_1 \le \sqrt{\alpha}$

\begin{displaymath}
    x_1 \le \sqrt{\alpha} \implies \frac{1}{2}(x_0 + \frac{\alpha}{x_0}) \le \sqrt{\alpha} \implies x_0 + \frac{\alpha}{x_0} \le 2\sqrt{\alpha}
\end{displaymath}

\begin{displaymath}
    \implies x_0 - 2\sqrt{\alpha} \le -\frac{\alpha}{x_0} \implies x_0^2 - 2\sqrt{\alpha}x_0 \le -\alpha
\end{displaymath}

Luego las raíces de $x_0^2 - 2\sqrt{\alpha}x_0$ son $0$ y $2\sqrt{\alpha}$. Ahora $\alpha$ es positivo pues es el resultado de aproximar una raíz, al haber demostrado que el polinomio tiene raíces, quiere decir que va a tomar valores positivos, contrariamente a lo que mostramos en la desigualdad. ¡Absurdo! Por lo tanto vale que $x_1 > \sqrt{\alpha}$ $\square$\\

{\bf Caso n + 1: $P(n) \implies P(n + 1)$}\\
Sea $k = n + 1$. Queremos probar que $x_{n + 1} < x_n$. Nuestra hipótesis inductiva es $x_n < x_{n - 1}$. Esto además implica por lo visto en el caso base que $x_n > \sqrt{\alpha}$.\\

Entonces:
\begin{displaymath}
    x_{n + 1} = \frac{1}{2}(x_n + \frac{\alpha}{x_n}) = \frac{x_n^2 + \alpha x_n}{2x_n} > \frac{x_n^2 + x_n^2}{2x_n} = x_n
\end{displaymath}

Esto vale porque $x_n > \sqrt{\alpha} \implies x_n^2 > \alpha$. Por lo tanto $x_{n + 1} < x_n$ $\square$


% Contendra una breve explicacion de la base teorica que fundamenta los metodos
% involu- crados en el trabajo, junto con los metodos mismos. No deben
% incluirse demostraciones de propiedades ni teoremas, ejemplos innecesarios,
% ni deniciones elementales (como por ejemplo la de matriz simetrica). En vez
% de deniciones basicas es conveniente citar ejemplos de bibliografa
% adecuada.  Una cita vale mas que mil palabras