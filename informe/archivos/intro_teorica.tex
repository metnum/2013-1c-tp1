\section{Introducción Teórica}

Las funciones que usaremos para aproximar la inversa de la raíz son las siguientes:

\begin{displaymath}
    f(x) = x^2 - \alpha
\end{displaymath}

\begin{displaymath}
    e(x) = \frac{1}{x^2} - \alpha
\end{displaymath}

A partir de este momento nos enfocaremos en encontrar los ceros de estas dos funciones usando los métodos de Newton y de la Secante:

\begin{displaymath}
    x_{n + 1} = x_n - \frac{h(x_n)}{h'(x_n)}
\end{displaymath}

\begin{displaymath}
    x_{n + 1} = x_{n - 1} - h(x_{n - 1})\frac{x_{n - 1} - x_{n - 2}}{h(x_{n - 1}) - h(x_{n - 2})}
\end{displaymath}

Donde $\displaystyle h(x) = f(x)$ o $\displaystyle h(x) = e(x)$

En este caso para calcular la inversa de la raíz cuadrada
($\displaystyle\frac{1}{\sqrt{\alpha}}$) buscaremos los ceros de dos funciones.

La resolución de estas dos funciones es equivalente a resolver el problema de la
inversa de la raíz. A partir de este planteo
podemos hacer uso de métodos para encontrar ceros de funciones.\\

Cada una de estas funciones provee formas de encontrar la raiz de un número.
Veamos el caso de $f(x)$.

Los métodos elegidos para encontrar las raices de estas funciones son el método
de Newton y el método de la Secante.

\subsubsection{Análisis de las funciones}

Notemos que $\alpha > 0$, dado que si no estará fuera del dominio.

\subsubsection{f(x)}


%Primero vamos a demostrar que cuando $f(x) = 0 \implies x = \sqrt{\alpha}$
%entonces una vez encontradas las raices podemos hacer $\frac{1}{x}$ para encontrar
%$\displaystyle\frac{1}{\sqrt{\alpha}}$.

%En el caso de $\displaystyle e(x) = 0 \implies x = \frac{1}{\sqrt{\alpha}}$ por lo que no tenemos que hacer
%ninguna otra cuenta.\\

%Analicemos graficamente las funciones:\\

% intertart grafico lindo de f(x)

$f(x)$ es una parábola. Al ser $\alpha > 0$ podemos ver que tiene dos
raíces. Más aún $f(x)$ es simétrica, por lo cual es posible encontrar cualquiera de
las dos raíces y con esta cambiarle el signo y obtener la otra. De esta forma
no nos preocuparemos por obtener la raíz positiva ya que nos es indistinto que
raíz conseguimos con los métodos.\\

% insertar grafico lindo de e(x)

$e(x)$ con $\alpha > 0$ también tiene siempre dos raíces por lo que al igual que con
$f(x)$ nos es indistinto cuál de las dos obtenemos. En este caso es importante
notar que en el 0, hay una asíntota de las ordenadas.\\
 
Los dos métodos que elegimos trabajan con la tangente de las funciones en un
punto o con una aproximación de esta.

% a partir de aca es todo chamuyo... ver que dejar y que sacar, porque en
% realida se puede hacer buen analisis analitico pero no se como.. buscar en
% google o wikipedia quizas


% graficos lindos de f''(x) y e''(x).

Veamos que $f''(x) == 2$. esta funcion es convexa. al ser constante y por la
forma que tienen las derivadas asumimos que siempre va a converger.

Observamos $e''(x) == 6/x^4$. Esto quiere decir que la misma tiene una asíntota en el 0,
y una pendiente que se agudiza rápidamente cerca de 0, lo cual puede dificultar la convergencia,
dado que para que las operaciones aritméticas involucradas en los métodos de aproximación 
den resultados de buena precisión, dependen de trabajar con números bien representables en punto flotante.

Cómo se puede ver los valores que hacen cero a esas funciones son de la forma
$x = \sqrt{\alpha}$ en el primer caso y $x = \frac{1}{\sqrt{\alpha}}$ en el
segundo.

\subsubsection{Métodos}
Tanto el método de Newton como el de Secante se pueden utilizar para resolver ambas funciones,
en particular porque, para Newton, existen las derivada primeras de las funciones.

\subsubsection{Hipótesis}
Creemos que Newton va a funcionar mucho mejor que Secante, porque su órden de
convergencia teórico es mejor.

Por otro lado no estamos seguros de cuál de las dos funciones va a ser mejor
porque no podemos interpretar funciones y como afectan las tangentes a los
métodos.

% no se si esto viene aca o mas abajo, quizas mas abajo o arriba, pero si nos
% interesa ver los dos casos, el caso con bajo error y el caso rapido.
Por último queremos hacer un análisis para un cálculo aproximado muy rápido,
como lo necesario para aplicaciones de alto rendimiento. Para este puede ser que las conclusiones sean
diferentes a casos donde puede haber mas iteraciones, o se arranque con aproximaciones buenas del resultado,
o hayan criterios más acotados sobre los que tiene que funcionar.

\subsection{Prueba de la convergencia de Newton}
Vamos a demostrar inductivamente la convergencia del método de Newton usando la función $\displaystyle f(x) = x^2 - \alpha$\\

Lo que queremos demostrar es que si $x_0 > \sqrt{\alpha}$ entonces $x_{n + 1} < x_n$ para todo $n > 0$ donde $\displaystyle x_{n + 1} = \frac{1}{2}(x_n + \frac{\alpha}{x_n})$.\\

{\large \bf Inducción en k}\\
Queremos probar que $x_{k + 1} < x_k$.\\

{\bf Caso base: $P(0)$}\\
Sea $k = 1$, queremos ver que $x_1 < x_0$. Supongamos que no vale. Entonces $x_1 \ge x_0$. Por lo tanto:

\begin{displaymath}
    \frac{1}{2}(x_0 + \frac{\alpha}{x_0}) \ge x_0 \implies x_0 + \frac{\alpha}{x_0} \ge 2x_0 \implies \frac{\alpha}{x_0} \ge x_0 \implies \alpha \ge x_0^2 \implies x_0 \le \sqrt{\alpha}
\end{displaymath}

Pero esto es absurdo pues por hipótesis sabíamos que $x_0 > \sqrt{\alpha}$. Por lo tanto $x_1 < x_0$ $\square$\\

Veamos también que $x_1 > \sqrt{\alpha}$\\

\begin{displaymath}
    x_1 > \sqrt{\alpha} \iff \frac{1}{2}(x_0 + \frac{\alpha}{x_0}) > \sqrt{\alpha} \iff x_0 + \frac{\alpha}{x_0} > 2\sqrt{\alpha}
\end{displaymath}

\begin{displaymath}
    \iff x_0 - 2\sqrt{\alpha} > -\frac{\alpha}{x_0} \iff x_0^2 - 2\sqrt{\alpha}x_0 + \alpha > 0 \iff (x_0 - \sqrt{\alpha})^2 > 0
\end{displaymath}

Luego esto último vale pues $x_0 > \sqrt{\alpha}$. Por lo tanto, por esto y por lo demostrado antes vale que:

\begin{displaymath}
    \sqrt{\alpha} < x_1 < x_0
\end{displaymath}

{\bf Caso n + 1: $P(n) \implies P(n + 1)$}\\
Sea $k = n + 1$. Queremos probar que $x_{n + 1} < x_n$. Nuestra hipótesis inductiva es $x_n < x_{n - 1}$. Esto además implica por lo visto en el caso base que $x_n > \sqrt{\alpha}$.\\

Entonces:
\begin{displaymath}
    x_{n + 1} = \frac{1}{2}(x_n + \frac{\alpha}{x_n}) = \frac{x_n^2 + \alpha}{2x_n} < \frac{x_n^2 + x_n^2}{2x_n} = x_n
\end{displaymath}

Esto vale porque $x_n > \sqrt{\alpha} \implies x_n^2 > \alpha$. Por lo tanto $x_{n + 1} < x_n$ $\square$


% Contendra una breve explicacion de la base teorica que fundamenta los metodos
% involu- crados en el trabajo, junto con los metodos mismos. No deben
% incluirse demostraciones de propiedades ni teoremas, ejemplos innecesarios,
% ni deniciones elementales (como por ejemplo la de matriz simetrica). En vez
% de deniciones basicas es conveniente citar ejemplos de bibliografa
% adecuada.  Una cita vale mas que mil palabras