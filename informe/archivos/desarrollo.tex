\section{Desarrollo}
% Deben explicarse los metodos numericos que utilizaron y su aplicacion al
% problema concreto involucrado en el trabajo practico. Se deben mencionar los
% pasos que si- guieron para implementar los algoritmos, las dicultades que
% fueron encontrando y la descripcion de como las fueron resolviendo. Explicar
% tambien como fueron planteadas y realizadas las mediciones experimentales.
% Los ensayos fallidos, hipotesis y conjeturas equivocadas, experimentos y
% metodos malogrados deben gurar en esta seccion, con una breve explicacion
% de los motivos de estas fallas (en caso de ser conocidas)

\subsection{Implementación}

Hicimos una implementación que nos permite decidir paramétricamente lo siguiente:

\begin{itemize}
    \item Función $e$ o $f$
    \item Método Newton o Secante
    \item Criterio de parada
    \begin{itemize}
        \item Iteraciones
        \item Tiempo
        \item Error Absoluto
        \item Error Relativo
    \end{itemize}
    \item Eleccion de $x_0$ y $x_1$ ($x_1$ solo para la Secante)
\end{itemize}

Esto lo desarrollamos en \verb|C| y luego hicimos unos $helpers$ en Python para poder
crear experimentos y analizar los resultados. Cada corrida nos devuelve todos
los valores de los criterios de parada y las aproximaciones en cada iteracion.\\

Estos experimentos son replicables y los puede ver en el anexo.\\

El codigo se puede optimizar MUCHO, sobre todo en las cuentas que realiza el
cuerpo de cada metodo. Ahora hay una implementacion naif que replica el enfoque
analitico pero que a nivel fierro se puede mejorar.\\

Por otro lado, agregamos mucho overhead con la flexibilidad, pero podemos ver
que esto afecta a los dos metodos y funciones por igual por lo que no deberia
afectar las comparaciones y resultados.

\subsubsection{Métodos}
Tanto el método de Newton como el de Secante se pueden utilizar para resolver ambas funciones,
en particular porque, para Newton, existen las derivada primeras de las funciones.

\subsection{Convergencia}
empezamos con x0 fijos y vimos que tanto newton como secante siempre convergian
para f(x). % mentira, hacer alguas corridas, tengan fe Esto es tanto para
alphas muy chicos (cercanos al cero) como para alhpas muy grandes.

en cambio para e(x) vimos que nunca convergia, al menos las aproximaciones
crecian muy rapidamente, tanto que en un puñado de iteraciones superaban el
mayor número representable por punto flotante doble (devolvia inf). % mostrar
alguna corrida donde pase esto.

Entonces para f(x) decidimos utilizar un x0=alpha que siempre funciona. Newton
tiene una convergencia teorica muy rapida por lo que no deberia influir.

En el caso de e(x) es muy interesante notar como las aproximaciones se alternan
entre positivas y negativas cuando no converge. esto se debe a ???.

% todavia no encontramos un x0 y x1 para e(x) para que converja

empiricamente (diseñar y pensar exp) encontramos que si elejimos un x1 y x0
cerca de e(x) converge pero sino no.

un primer acercamiento a x0 que se nos ocurrio fue utilizar 0.0alpha y
0.00alpha. Esto funciona hasta alphas no muy grandes (mas o menos hasta $10^4$,
ajustar es numero con un experimento).

% explicar que es DBL EPSILON y que es DBL MIN
Al hacer estas pruebas nos parece que si x0 y x1 son menos que la raiz
converge. Entonce decidimos utilizar $DBL_MIN$ y $DBL_MIN * 2$ pero estas rompian
las operaciones aritmeticas de punto flotante. Entonce pasamos a utilizar
$DBL_EPSILON$ y encontramos que funciona muy bien tanto para numeros muy chicos
como para numeros muy grandes. El problema que tiene es que requiere de muchas
iteraciones.

Grafiquemos la cantidad de iteraciones necesarias para diferentes alphas (muy
chicos y cercanos al $1$ y muy grandes) para $DBL_EPSILON$. Lo podemos mejorar?
creemos que si, pero quedará para futura investigacion.

Otra cosa que se nos ocurrio es utilizar aproximaciones de f(x) primero hasta
un cierto error grosero y luego continuar refinando con e(x). Pero para que
esto tenga sentido tuvimos que hacer una comparacion entre los metodos. Ver mas
abajo.

